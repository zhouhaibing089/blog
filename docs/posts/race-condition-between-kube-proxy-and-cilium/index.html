<!doctype html>











































<html
  class="not-ready lg:text-base"
  style="--bg: #fbfbfb"
  lang="en-us"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>Race condition between kube-proxy and cilium - Hi-Been Space</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="This is an investigation story where all pods on one host lost connectivity to external service. It looks like something below:
$ nsenter --net=/proc/29080/ns/net curl docs.cilium.io -v &gt; GET / HTTP/1.1 &gt; Host: docs.cilium.io &gt; User-Agent: curl/7.79.1 &gt; Accept: */* &gt; Here 29080 is the pid of some pod process, and it appeared that TCP connection has been established, but somehow it couldn&rsquo;t receive any response.
About the cluster The cluster was set up like something below:" />
  <meta name="author" content="Haibing Zhou" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://blog.zhouhaibing.com/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="https://blog.zhouhaibing.com/theme.png" />

  
  
  
  
  <link rel="preload" as="image" href="https://www.gravatar.com/avatar/5efba87a944c70d1abe7f7892f4df74b?s=160&amp;d=identicon" />
  
  

  
  
  <link rel="preload" as="image" href="https://blog.zhouhaibing.com/github.svg" />
  
  <link rel="preload" as="image" href="https://blog.zhouhaibing.com/linkedin.svg" />
  
  

  
  
  <script
    defer
    src="https://blog.zhouhaibing.com/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  
  

  
  <link
    rel="icon"
    href="https://blog.zhouhaibing.com/favicon.ico"
  />
  <link
    rel="apple-touch-icon"
    href="https://blog.zhouhaibing.com/apple-touch-icon.png"
  />

  
  <meta name="generator" content="Hugo 0.130.0">

  
  
  
  
  


  
  
  <meta itemprop="name" content="Race condition between kube-proxy and cilium">
  <meta itemprop="description" content="This is an investigation story where all pods on one host lost connectivity to external service. It looks like something below:
$ nsenter --net=/proc/29080/ns/net curl docs.cilium.io -v &gt; GET / HTTP/1.1 &gt; Host: docs.cilium.io &gt; User-Agent: curl/7.79.1 &gt; Accept: */* &gt; Here 29080 is the pid of some pod process, and it appeared that TCP connection has been established, but somehow it couldn’t receive any response.
About the cluster The cluster was set up like something below:">
  <meta itemprop="datePublished" content="2023-05-27T16:00:00-07:00">
  <meta itemprop="dateModified" content="2023-05-27T16:00:00-07:00">
  <meta itemprop="wordCount" content="1534">
  
  <meta property="og:url" content="https://blog.zhouhaibing.com/posts/race-condition-between-kube-proxy-and-cilium/">
  <meta property="og:site_name" content="Hi-Been Space">
  <meta property="og:title" content="Race condition between kube-proxy and cilium">
  <meta property="og:description" content="This is an investigation story where all pods on one host lost connectivity to external service. It looks like something below:
$ nsenter --net=/proc/29080/ns/net curl docs.cilium.io -v &gt; GET / HTTP/1.1 &gt; Host: docs.cilium.io &gt; User-Agent: curl/7.79.1 &gt; Accept: */* &gt; Here 29080 is the pid of some pod process, and it appeared that TCP connection has been established, but somehow it couldn’t receive any response.
About the cluster The cluster was set up like something below:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-05-27T16:00:00-07:00">
    <meta property="article:modified_time" content="2023-05-27T16:00:00-07:00">

  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Race condition between kube-proxy and cilium">
  <meta name="twitter:description" content="This is an investigation story where all pods on one host lost connectivity to external service. It looks like something below:
$ nsenter --net=/proc/29080/ns/net curl docs.cilium.io -v &gt; GET / HTTP/1.1 &gt; Host: docs.cilium.io &gt; User-Agent: curl/7.79.1 &gt; Accept: */* &gt; Here 29080 is the pid of some pod process, and it appeared that TCP connection has been established, but somehow it couldn’t receive any response.
About the cluster The cluster was set up like something below:">

  
  

  
  <link rel="canonical" href="https://blog.zhouhaibing.com/posts/race-condition-between-kube-proxy-and-cilium/" />
  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a class="-translate-y-[1px] text-2xl" href="https://blog.zhouhaibing.com/"
      >Hi-Been Space</a
    >
    <div
      class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#fbfbfb'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 dark:invert lg:ml-14 lg:mt-0 lg:items-center"
    >
      
      <a
        class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/zhouhaibing089"
        target="_blank"
        rel="me"
      >
        github
      </a>
      
      <a
        class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./linkedin.svg)"
        href="https://linkedin.com/in/haibzhou"
        target="_blank"
        rel="me"
      >
        linkedin
      </a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-14 dark:prose-invert"
    >
      

<article>
  <header class="mb-16">
    <h1 class="!my-0 pb-2.5">Race condition between kube-proxy and cilium</h1>

    
    <div class="text-xs antialiased opacity-60">
      
      <time>May 27, 2023</time>
      
      
      
      
    </div>
    
  </header>

  <section><p>This is an investigation story where all pods on one host lost connectivity to
external service. It looks like something below:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>$ nsenter --net<span style="color:#f92672">=</span>/proc/29080/ns/net curl docs.cilium.io -v
</span></span><span style="display:flex;"><span>&gt; GET / HTTP/1.1
</span></span><span style="display:flex;"><span>&gt; Host: docs.cilium.io
</span></span><span style="display:flex;"><span>&gt; User-Agent: curl/7.79.1
</span></span><span style="display:flex;"><span>&gt; Accept: */*
</span></span><span style="display:flex;"><span>&gt;
</span></span></code></pre></div><p>Here <strong>29080</strong> is the pid of some pod process, and it appeared that TCP
connection has been established, but somehow it couldn&rsquo;t receive any response.</p>
<h2 id="about-the-cluster">About the cluster</h2>
<p>The cluster was set up like something below:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>$ minikube start --driver<span style="color:#f92672">=</span>virtualbox --cpus<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span> --memory<span style="color:#f92672">=</span>8g --network-plugin<span style="color:#f92672">=</span>cni <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --cni=false --nodes=3
</span></span></code></pre></div><p>and then cilium was installed with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>$ cilium install --kube-proxy-replacement<span style="color:#f92672">=</span>probe <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --helm-set ipam.operator.clusterPoolIPv4PodCIDR=172.16.0.0/16 \
</span></span><span style="display:flex;"><span>    --helm-set bpf.masquerade=true \
</span></span><span style="display:flex;"><span>    --version=v1.11.17
</span></span></code></pre></div><p>In short, this is a cluster created with minikube, and then cilium was
installed as the CNI plugin, and as kube-proxy replacement whenever possible.</p>
<h2 id="about-the-pod-network">About the pod network</h2>
<p>Below is a basic representation of the network setup for the pod which had
connectivity issues:</p>
<p><img src="/images/kube-proxy-cilium.svg" alt="pod-node-network"></p>
<p>There is a veth pair with one end in pod and the other end in host. It basically
connects the pod network namespace to host network namespace. There is another
veth pair with both ends in host (<code>cilium_net</code> and <code>cilium_host</code>).</p>
<h2 id="tcpdump">tcpdump</h2>
<p>To understand why the pod couldn&rsquo;t receive anything back, we can run <code>tcpdump</code>
to analyze the traffic flow. To start with, we can try pod network namespace
first:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>$ nsenter --net<span style="color:#f92672">=</span>/proc/29080/ns/net tcpdump -i any host 104.17.32.82 -nn
</span></span><span style="display:flex;"><span>23:43:46.468314 eth0  Out IP 172.16.0.30.50880 &gt; 104.17.32.82.80: Flags [S], seq 2499115150, win 64860, options [mss 1410,sackOK,TS val 239543031 ecr 0,nop,wscale 7], length 0
</span></span><span style="display:flex;"><span>23:43:46.478847 eth0  In  IP 104.17.32.82.80 &gt; 172.16.0.30.50880: Flags [S.], seq 881408001, ack 2499115151, win 65535, options [mss 1460], length 0
</span></span><span style="display:flex;"><span>23:43:46.478867 eth0  Out IP 172.16.0.30.50880 &gt; 104.17.32.82.80: Flags [.], ack 1, win 64860, length 0
</span></span><span style="display:flex;"><span>23:43:46.478892 eth0  Out IP 172.16.0.30.50880 &gt; 104.17.32.82.80: Flags [P.], seq 1:79, ack 1, win 64860, length 78: HTTP: GET / HTTP/1.1
</span></span><span style="display:flex;"><span>23:43:46.693746 eth0  Out IP 172.16.0.30.50880 &gt; 104.17.32.82.80: Flags [P.], seq 1:79, ack 1, win 64860, length 78: HTTP: GET / HTTP/1.1
</span></span><span style="display:flex;"><span>23:43:47.117837 eth0  Out IP 172.16.0.30.50880 &gt; 104.17.32.82.80: Flags [P.], seq 1:79, ack 1, win 64860, length 78: HTTP: GET / HTTP/1.1
</span></span><span style="display:flex;"><span>23:43:54.143858 eth0  In  IP 104.17.32.82.80 &gt; 172.16.0.30.50880: Flags [S.], seq 881408001, ack 2499115151, win 65535, options [mss 1460], length 0
</span></span><span style="display:flex;"><span>23:43:54.143877 eth0  Out IP 172.16.0.30.50880 &gt; 104.17.32.82.80: Flags [.], ack 1, win 64860, length 0
</span></span></code></pre></div><p>To make it more readable:</p>
<ol>
<li>172.16.0.30.50880 (pod) -&gt; 104.17.32.82.80 (server): SYN</li>
<li>104.17.32.82.80 (server) -&gt; 172.16.0.30.50880 (pod): SYN and ACK</li>
<li>172.16.0.30.50880 (pod) -&gt; 104.17.32.82.80 (server): ACK</li>
<li>172.16.0.30.50880 (pod) -&gt; 104.17.32.82.80 (server): PUSH and ACK</li>
</ol>
<p>Step 4 was retried several times, until there is another SYN and ACK replied
from server. This mostly means that the server didn&rsquo;t see the ACK reply from
step 3, and thus retried step 2, but then what happened to the ACK in step 3?</p>
<p>We can then run the same tcpdump command from host, and see what happened in
host network namespace:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>$ tcpdump -i any host 104.17.32.82 -nn
</span></span><span style="display:flex;"><span>23:53:50.103850 lxc3055a222b7f1 In  IP 172.16.0.30.50992 &gt; 104.17.32.82.80: Flags [S], seq 2399693262, win 64860, options [mss 1410,sackOK,TS val 240146666 ecr 0,nop,wscale 7], length 0
</span></span><span style="display:flex;"><span>23:53:50.103919 eth0  Out IP 10.0.2.15.50992 &gt; 104.17.32.82.80: Flags [S], seq 2399693262, win 64860, options [mss 1410,sackOK,TS val 240146666 ecr 0,nop,wscale 7], length 0
</span></span><span style="display:flex;"><span>23:53:50.115561 eth0  In  IP 104.17.32.82.80 &gt; 10.0.2.15.50992: Flags [S.], seq 958144001, ack 2399693263, win 65535, options [mss 1460], length 0
</span></span><span style="display:flex;"><span>23:53:50.115586 lxc3055a222b7f1 In  IP 172.16.0.30.50992 &gt; 104.17.32.82.80: Flags [.], ack 958144002, win 64860, length 0
</span></span><span style="display:flex;"><span>23:53:50.115677 lxc3055a222b7f1 In  IP 172.16.0.30.50992 &gt; 104.17.32.82.80: Flags [P.], seq 0:78, ack 1, win 64860, length 78: HTTP: GET / HTTP/1.1
</span></span></code></pre></div><p>From host perspective, we can observe traffic coming from <code>lxc3055a222b7f1</code>
which is the host end of pod veth pair. Here the first SYN packet actually was
sent out from <code>eth0</code> interface, but none of the other packets were sent out via
<code>eth0</code>. The <code>ACK</code> packet replied by pod clearly was lost somewhere.</p>
<h2 id="iptables">iptables</h2>
<p>Here we can make few guesses:</p>
<ol>
<li>It is more likely dropped by iptables than bpf.</li>
<li>It is more likely related to conntrack iptables rules.</li>
</ol>
<p>The second guess is based on the fact that first SYN packet was sent out without
any issues. To examine this guess, we can find out all the iptables rules which
can drop packets based on conntrack states via:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>$ iptables -S | grep DROP | grep <span style="color:#e6db74">&#34;ctstate&#34;</span>
</span></span><span style="display:flex;"><span>-A KUBE-FIREWALL ! -s 127.0.0.0/8 -d 127.0.0.0/8 -m comment --comment &#34;block incoming localnet connections&#34; -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP
</span></span><span style="display:flex;"><span>-A KUBE-FORWARD -m conntrack --ctstate INVALID -j DROP
</span></span></code></pre></div><p>The first rule doesn&rsquo;t matter because <code>-d 127.0.0.0/8</code> isn&rsquo;t our case, so let&rsquo;s
focus on the second rule.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>-A KUBE-FORWARD -m conntrack --ctstate INVALID -j DROP
</span></span></code></pre></div><p>This rule says that if <code>ctstate</code> is INVALID, then the packet should be dropped.
We can find the place of this rule fairly straightforward:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span># iptables -t filter -nvL KUBE-FORWARD
</span></span><span style="display:flex;"><span>Chain KUBE-FORWARD (1 references)
</span></span><span style="display:flex;"><span> pkts bytes target     prot opt in     out     source               destination
</span></span><span style="display:flex;"><span>    5   512 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate INVALID
</span></span><span style="display:flex;"><span>    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding rules */ mark match 0x4000/0x4000
</span></span><span style="display:flex;"><span>    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding conntrack rule */ ctstate RELATED,ESTABLISHED
</span></span></code></pre></div><p>Here we can inspect the dropped <code>pkts/bytes</code> by running the tests again, and it
isn&rsquo;t difficult to confirm that it is indeed this rule which dropped our
packets.</p>
<p>Starting from here, we have two questions to ask:</p>
<ol>
<li>How does it look like from a good host?</li>
<li>Why the conntrack state is INVALID?</li>
</ol>
<h2 id="compare-with-good-host">Compare with good host</h2>
<p>This is also straightforward. <code>KUBE-FOREWARD</code> chain can only be jumpped from
<code>FORWARD</code> chain, so let&rsquo;s just compare how does it look like in <code>FORWARD</code> chain:</p>
<p>On a bad host:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>$ iptables -t filter -nvL FORWARD
</span></span><span style="display:flex;"><span>Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
</span></span><span style="display:flex;"><span> pkts bytes target     prot opt in     out     source               destination
</span></span><span style="display:flex;"><span>   11   660 KUBE-PROXY-FIREWALL  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes load balancer firewall */
</span></span><span style="display:flex;"><span>  169 14804 KUBE-FORWARD  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding rules */
</span></span><span style="display:flex;"><span>   11   660 KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes service portals */
</span></span><span style="display:flex;"><span>   11   660 KUBE-EXTERNAL-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes externally-visible service portals */
</span></span><span style="display:flex;"><span>   11   660 DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0
</span></span><span style="display:flex;"><span>   11   660 DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0
</span></span><span style="display:flex;"><span>    0     0 ACCEPT     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
</span></span><span style="display:flex;"><span>    0     0 DOCKER     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0
</span></span><span style="display:flex;"><span>    0     0 ACCEPT     all  --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0
</span></span><span style="display:flex;"><span>    0     0 ACCEPT     all  --  docker0 docker0  0.0.0.0/0            0.0.0.0/0
</span></span><span style="display:flex;"><span>    7   420 CILIUM_FORWARD  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* cilium-feeder: CILIUM_FORWARD */
</span></span></code></pre></div><p>and on a good host:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>$ iptables -t filter -nvL FORWARD
</span></span><span style="display:flex;"><span>Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
</span></span><span style="display:flex;"><span> pkts bytes target     prot opt in     out     source               destination
</span></span><span style="display:flex;"><span> 1419  148K CILIUM_FORWARD  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* cilium-feeder: CILIUM_FORWARD */
</span></span><span style="display:flex;"><span>    0     0 KUBE-PROXY-FIREWALL  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes load balancer firewall */
</span></span><span style="display:flex;"><span>    0     0 KUBE-FORWARD  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding rules */
</span></span><span style="display:flex;"><span>    0     0 KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes service portals */
</span></span><span style="display:flex;"><span>    0     0 KUBE-EXTERNAL-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes externally-visible service portals */
</span></span><span style="display:flex;"><span>    0     0 DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0
</span></span><span style="display:flex;"><span>    0     0 DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0
</span></span><span style="display:flex;"><span>    0     0 ACCEPT     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
</span></span><span style="display:flex;"><span>    0     0 DOCKER     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0
</span></span><span style="display:flex;"><span>    0     0 ACCEPT     all  --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0
</span></span><span style="display:flex;"><span>    0     0 ACCEPT     all  --  docker0 docker0  0.0.0.0/0            0.0.0.0/0
</span></span></code></pre></div><p>The difference here is that <code>CILIUM_FORWARD</code> is matched first. We can see what&rsquo;s
in <code>CILIUM_FORWARD</code> with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>$ iptables -t filter -nvL CILIUM_FORWARD
</span></span><span style="display:flex;"><span>Chain CILIUM_FORWARD (1 references)
</span></span><span style="display:flex;"><span> pkts bytes target     prot opt in     out     source               destination
</span></span><span style="display:flex;"><span>  642 93820 ACCEPT     all  --  *      cilium_host  0.0.0.0/0            0.0.0.0/0            /* cilium: any-&gt;cluster on cilium_host forward accept */
</span></span><span style="display:flex;"><span>    0     0 ACCEPT     all  --  cilium_host *       0.0.0.0/0            0.0.0.0/0            /* cilium: cluster-&gt;any on cilium_host forward accept (nodeport) */
</span></span><span style="display:flex;"><span>  794 56164 ACCEPT     all  --  lxc+   *       0.0.0.0/0            0.0.0.0/0            /* cilium: cluster-&gt;any on lxc+ forward accept */
</span></span><span style="display:flex;"><span>    0     0 ACCEPT     all  --  cilium_net *       0.0.0.0/0            0.0.0.0/0            /* cilium: cluster-&gt;any on cilium_net forward accept (nodeport) */
</span></span></code></pre></div><p>Here, the packet is accepted if the packet is sent from <code>lxc+</code>, and thus the
packet won&rsquo;t continue matching with <code>KUBE-FORWARD</code>.</p>
<h2 id="how-did-it-happen">How did it happen</h2>
<p>We can look at the code in kube-proxy and cilium, and it won&rsquo;t take too long to
realize that the order of <code>CILIUM_FORWARD</code> and <code>KUBE-FORWARD</code> relies on which
component starts first initially. In a typical case, if cilium starts later than
kube-proxy, <code>CILIUM_FORWARD</code> is going to be prepended and thus takes higher
precedence. However if kube-proxy starts after cilium, then <code>KUBE-FORWARD</code> will
take precedence, and in this case, it breaks pod connectivity.</p>
<h2 id="why-ctstate-is-invalid">Why ctstate is INVALID</h2>
<p>iptables tracks connection states, but if only part of traffic goes through
iptables, then it won&rsquo;t recognize the connection states correctly. In this case
here, we have <code>bpf.masquerade=true</code> configured in cilium. It basically means
cilium actually does snat via bpf directly (in contrast to doing it in
iptables). With that said, when iptables sees there is an ACK without knowing
there was a SYN before, it is considered as INVALID.</p>
</section>

  
  

  
  
  
  
  <nav
    class="mt-24 flex rounded-lg bg-black/[3%] text-lg !leading-[1.2] dark:bg-white/[8%]"
  >
    
    <a
      class="flex w-1/2 items-center rounded-l-md p-6 pr-3 font-medium no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://blog.zhouhaibing.com/posts/etcd-clustering-step-by-step/"
      ><span class="mr-1.5">←</span><span>etcd clustering step by step</span></a
    >
    
    
    <a
      class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 font-medium no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://blog.zhouhaibing.com/posts/kubectl-alias-done-right/"
      ><span>kubectl alias done right</span><span class="ml-1.5">→</span></a
    >
    
  </nav>
  
  

  
  
  <div class="mt-24" id="disqus_thread"></div>
  <script>
    const disqusShortname = 'zhouhaibing089';
    const script = document.createElement('script');
    script.src = 'https://' + disqusShortname + '.disqus.com/embed.js';
    script.setAttribute('data-timestamp', +new Date());
    document.head.appendChild(script);
  </script>
  

  
  

  


  
</article>


    </main>

    <footer
  class="mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-xs uppercase tracking-wider opacity-60"
>
  <div class="mr-auto">
    &copy; 2024
    <a class="link" href="https://blog.zhouhaibing.com/">Hi-Been Space</a>
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >powered by hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >hugo-paper</a
  >
</footer>

  </body>
</html>
